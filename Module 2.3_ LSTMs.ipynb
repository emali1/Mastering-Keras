{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Module 2.3: LSTMs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSDYSPCLIpsR"
      },
      "source": [
        "## Module 2.3: Working with LSTMs in Keras (A Review)\n",
        "\n",
        "We turn to implementing a type of recurrent neural network know as LSTM in the Keras functional API. In this module we will pay attention to:\n",
        "\n",
        "1. Using the Keras functional API for defining models.\n",
        "2. Mounting your Google drive to your Colab environment for file interface.\n",
        "3. Generating synthetic data from a LSTM and sequence seed.\n",
        "\n",
        "Those students who are comfortable with all these matters might consider skipping ahead.\n",
        "\n",
        "Note that we will not spend time tuning hyper-parameters: The purpose is to show how different techniques can be implemented in Keras, not to solve particular data science problems as optimally as possible. Obviously, most techniques include hyper-parameters that need to be tuned for optimal performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TK1JgxD3LD1U"
      },
      "source": [
        "First we import required libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHKulE0YrxuG",
        "outputId": "809d7e5f-5060-4014-87e1-63dabce666c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Apr 20 09:52:10 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    24W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btVPO4ETIEfL"
      },
      "source": [
        "import sys\n",
        "import numpy\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras import Model\n",
        "from keras.optimizers import Adadelta\n",
        "from keras.layers import Dense,Dropout,LSTM,Input\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGCD1zv5Low8"
      },
      "source": [
        "We will have a little fun and try to teach a neural network to write like Lewis Carroll, the author of Alice in Wonderland.\n",
        "\n",
        "Note, though, that the same technique can be used to model any sequential system, and generate simulations from seeds for such a system. Here the sequence are the characters written by Carroll during Alice in Wonderland, but it could be, for example, an industrial system that evolves in time. In that case, when we generate simulations of the system based on current and recent conditions we simulate the expected evolution of the system - something of great value!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfYWdsSGQJlQ"
      },
      "source": [
        "We will use the [Project Gutenburg text file of Alice in Wonderland](https://www.gutenberg.org/files/11/11.txt). But we need to get the file into our colab environment and this takes some work.\n",
        "\n",
        "First, you need to place the file in your google drive. We will assume that you will place it in a folder called \"Mastering Keras Datasets\", and that you rename it \"Alice.txt\". If you don't, you will need to the file path used in the code.\n",
        "\n",
        "Once you have done that, you will need to mount your google drive in Colab. Run the following code and complete the required authorizations.\n",
        "\n",
        "Note that you will need to mount your drive every time you use code from this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlKpjJfKcksv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4abcdea-1f8a-48f5-e1aa-b34345aa8316"
      },
      "source": [
        "# Note: You will need to mount your drive every time you \n",
        "# run code in this tutorial.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oV3vUZryMUjT"
      },
      "source": [
        "Now we can load the file using code and prepare the data. We want to work with sequences of 100 characters as input data, and our target will be the next (101st) character.\n",
        "\n",
        "To keep things simple, we will ignore upper/lower case character distinctions, and cast all alphabetical characters to lower case. To allow our model to work with these characters, we will encode them as integers. We will then normalize them to real numbers between 0 and 1 and add a dimension (we are working with a system with a single feature). Finally we will one-hot encode the target character (see previous module for discussion of one-hot encoding). This is not the only way to handle the data, but it is a simple one.\n",
        "\n",
        "We will also return the unnormalized and non-reshaped X data, the number of characters found and an integer coding to character dictionary, all for use later.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RXtz4LwLHoY"
      },
      "source": [
        "def load_alice (\n",
        "    rawTextFile=\"/content/drive/My Drive/Mastering Keras/Alice.txt\"   \n",
        "    ):\n",
        "    # load ascii text and covert to lowercase\n",
        "    raw_text = open(rawTextFile, encoding='utf-8').read()\n",
        "    raw_text = raw_text.lower()\n",
        "    # create mapping of unique chars to integers\n",
        "    chars = sorted(list(set(raw_text)))\n",
        "    char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "    int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "    # summarize the loaded data\n",
        "    n_chars = len(raw_text)\n",
        "    n_vocab = len(chars)\n",
        "    print (\"Total Characters: \", n_chars)\n",
        "    print (\"Total Vocab: \", n_vocab)\n",
        "    # prepare the dataset of input to output pairs encoded as integers\n",
        "    seq_length = 100\n",
        "    dataX = []\n",
        "    dataY = []\n",
        "    for i in range(0, n_chars - seq_length, 1):\n",
        "    \tseq_in = raw_text[i:i + seq_length]\n",
        "    \tseq_out = raw_text[i + seq_length]\n",
        "    \tdataX.append([char_to_int[char] for char in seq_in])\n",
        "    \tdataY.append(char_to_int[seq_out])\n",
        "    n_patterns = len(dataX)\n",
        "    print (\"Total Patterns: \", n_patterns)\n",
        "    # reshape X to be [samples, time steps, features]\n",
        "    X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "    # normalize\n",
        "    X = X / float(n_vocab)\n",
        "    # one hot encode the output variable\n",
        "    Y = np_utils.to_categorical(dataY)\n",
        "    return X,Y,dataX,n_vocab,int_to_char"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LikRHGnkM8r-"
      },
      "source": [
        "Now lets load the data. X and Y are the input and target label datasets we will use in training. X_ is the un-reshaped X data for use later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXdKny5NM_RT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fd7294f-9745-4c64-cf4a-e7895b11c3a3"
      },
      "source": [
        "X,Y,X_,n_vocab,int_to_char = load_alice()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  164047\n",
            "Total Vocab:  64\n",
            "Total Patterns:  163947\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHOixFxYdl1H"
      },
      "source": [
        "You can play around below to look at the shape of the resulting X and Y arrays, as well as their contents. But they are no longer understandable character strings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2fL5Qbjdk1Z"
      },
      "source": [
        "# Play around here to look at data characteristics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3Wk5Dvzexjs"
      },
      "source": [
        "Now we define our LSTM using the Keras function API. We are going to make use of LSTM layers, and add a dropout layer for regularization.\n",
        "\n",
        "We will pass the data to the model defining function so that we can read input and output dimensions of it, rather than hard coding them.\n",
        "\n",
        "For comparison, a second version of the function is included showing how to use the sequential approach."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpQik6VXIdTl"
      },
      "source": [
        "def get_model (X,Y):\n",
        "    # define the LSTM model\n",
        "    inputs=Input(shape=(X.shape[1],X.shape[2]),name=\"Input\")\n",
        "    lstm1=LSTM(256, input_shape=(100,1),return_sequences=True)(inputs)\n",
        "    drop1=Dropout(0.2)(lstm1)\n",
        "    lstm2=LSTM(256)(drop1)\n",
        "    drop2=Dropout(0.2)(lstm2)\n",
        "    outputs=Dense(Y.shape[1], activation='softmax')(drop2)\n",
        "    model=Model(inputs=inputs,outputs=outputs)\n",
        "    return model\n",
        "\n",
        "def get_model_sequential (X,Y):\n",
        "    # define the LSTM model\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(256, input_shape=(X.shape[1],X.shape[2]),return_sequences=True))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(256))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(Y.shape[1], activation='softmax'))\n",
        "    return model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25ea1_kwhKSL"
      },
      "source": [
        "We get our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHO2WNkIhPz4"
      },
      "source": [
        "model=get_model(X,Y)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8AlPqyChSLp"
      },
      "source": [
        "Now we will define an optimizer and compile it. If you are unfamiliar with the different types of optimizers available in keras, I suggest you read the keras documentation [here](https://keras.io/optimizers/) and play around training the model with different alternatives."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4QI98iFhdRn"
      },
      "source": [
        "opt=Adadelta()\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNvQlYwUhhps"
      },
      "source": [
        "And we compile our model with the optimizer ready for training. We use categorical crossentropy as our loss function as this is a good default choice for working with a multi-class categorical target variable (i.e. the next character labels)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gBX1zHbh8T_"
      },
      "source": [
        "model.compile(optimizer=opt,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxHPgLnEkZdn"
      },
      "source": [
        "Now we will make a function to fit the model. We will not do this very professionally (it is just a fun project), and so will not use any validation data. Rather, we will just run the training for a number of epoches - by default 20, though you can change this.\n",
        "\n",
        "We will, though, use a ModelCheckpoint callback to save the best performing weights and load these into the model and the conclusion of the training. Note that training performance should normally improve with more epoches, so this is unlikely to improve performance. What we really want is to be able to load the best weights without having to redo the training process (see below)\n",
        "\n",
        "If you want to, you are encouraged to alter the code in this tutorial to work with a training and validation set, and adjust the fit function below to incorporate an EarlyStopping callback based on performance on the validation data.\n",
        "\n",
        "We have two one LSTM layer, we are dealing with sequences of length 100. So if we 'unroll' it, we have a network of 200 LSTM layers. And inside these layers are infact multiple internal layers setting up the LSTM architecture! So this is actually a pretty big network, and training will take some time (about 200 hours on the free Colab environment for 200 epochs). This is probably too much to conveniently run yourself.\n",
        "\n",
        "Here we have an example of how we could train it on Colab. Colab will eventually time out. The best thing to do is to save our weights file to our google drive, so we can load it at leisure later and resume training. This is what we will do. Remember that if you didn't use the default name for your folder in your google drive you should change the path string in the code.\n",
        "\n",
        "In real life, you will also often want to save the state of the optimizer (so that it keeps its current learning rate, etc). You can do this by accessing and saving model.optimizer.get_state(). It is left as an exercise to implement this.\n",
        "\n",
        "*It is not expected that you train the network using this function - see below to load trained weights from your google drive.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2W1MXN18kZ3N"
      },
      "source": [
        "def fit_model (model,X,Y,epochs=100):\n",
        "    # define the checkpoint callback\n",
        "    filepath=\"/content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\" \n",
        "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, \n",
        "                                 save_best_only=True, mode='min')\n",
        "    callbacks_list = [checkpoint]\n",
        "    # fit the model\n",
        "    model.fit(X, Y, epochs=epochs, batch_size=128, callbacks=callbacks_list)\n",
        "    # load the best weights\n",
        "    model.load_weights(filename)\n",
        "    # return the final model\n",
        "    return model\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ek8_BWfrh8fm"
      },
      "source": [
        "We would then fit (train) the model by calling the above function.\n",
        "\n",
        "*It is not expected that you train the network using this function - see below to load trained weights from your google drive.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzsCLeYthGK-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cd28ddc2-26be-4dfa-84c7-e031d5aed89b"
      },
      "source": [
        "model=fit_model(model,X,Y,100)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1281/1281 [==============================] - 44s 20ms/step - loss: 4.1304 - accuracy: 0.0465\n",
            "\n",
            "Epoch 00001: loss improved from inf to 4.11652, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 2/100\n",
            "1281/1281 [==============================] - 26s 20ms/step - loss: 4.0462 - accuracy: 0.1652\n",
            "\n",
            "Epoch 00002: loss improved from 4.11652 to 3.97929, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 3/100\n",
            "1281/1281 [==============================] - 26s 20ms/step - loss: 3.5387 - accuracy: 0.1664\n",
            "\n",
            "Epoch 00003: loss improved from 3.97929 to 3.43717, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 4/100\n",
            "1281/1281 [==============================] - 26s 21ms/step - loss: 3.2842 - accuracy: 0.1676\n",
            "\n",
            "Epoch 00004: loss improved from 3.43717 to 3.26262, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 5/100\n",
            "1281/1281 [==============================] - 26s 21ms/step - loss: 3.2103 - accuracy: 0.1667\n",
            "\n",
            "Epoch 00005: loss improved from 3.26262 to 3.20185, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 6/100\n",
            "1281/1281 [==============================] - 26s 21ms/step - loss: 3.1790 - accuracy: 0.1641\n",
            "\n",
            "Epoch 00006: loss improved from 3.20185 to 3.17309, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 7/100\n",
            "1281/1281 [==============================] - 26s 21ms/step - loss: 3.1600 - accuracy: 0.1622\n",
            "\n",
            "Epoch 00007: loss improved from 3.17309 to 3.15813, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 8/100\n",
            "1281/1281 [==============================] - 26s 21ms/step - loss: 3.1478 - accuracy: 0.1635\n",
            "\n",
            "Epoch 00008: loss improved from 3.15813 to 3.14732, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 9/100\n",
            "1281/1281 [==============================] - 26s 21ms/step - loss: 3.1421 - accuracy: 0.1622\n",
            "\n",
            "Epoch 00009: loss improved from 3.14732 to 3.13951, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 10/100\n",
            "1281/1281 [==============================] - 26s 21ms/step - loss: 3.1370 - accuracy: 0.1631\n",
            "\n",
            "Epoch 00010: loss improved from 3.13951 to 3.13468, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 11/100\n",
            "1281/1281 [==============================] - 26s 21ms/step - loss: 3.1330 - accuracy: 0.1624\n",
            "\n",
            "Epoch 00011: loss improved from 3.13468 to 3.13222, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 12/100\n",
            "1281/1281 [==============================] - 26s 20ms/step - loss: 3.1280 - accuracy: 0.1626\n",
            "\n",
            "Epoch 00012: loss improved from 3.13222 to 3.12880, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 13/100\n",
            "1281/1281 [==============================] - 26s 21ms/step - loss: 3.1263 - accuracy: 0.1622\n",
            "\n",
            "Epoch 00013: loss improved from 3.12880 to 3.12587, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 14/100\n",
            "1281/1281 [==============================] - 26s 21ms/step - loss: 3.1250 - accuracy: 0.1614\n",
            "\n",
            "Epoch 00014: loss improved from 3.12587 to 3.12359, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 15/100\n",
            "1281/1281 [==============================] - 26s 21ms/step - loss: 3.1250 - accuracy: 0.1606\n",
            "\n",
            "Epoch 00015: loss improved from 3.12359 to 3.12168, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 16/100\n",
            "1281/1281 [==============================] - 26s 21ms/step - loss: 3.1207 - accuracy: 0.1629\n",
            "\n",
            "Epoch 00016: loss improved from 3.12168 to 3.12143, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 17/100\n",
            "1281/1281 [==============================] - 26s 21ms/step - loss: 3.1180 - accuracy: 0.1624\n",
            "\n",
            "Epoch 00017: loss improved from 3.12143 to 3.12082, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 18/100\n",
            "1281/1281 [==============================] - 26s 21ms/step - loss: 3.1174 - accuracy: 0.1612\n",
            "\n",
            "Epoch 00018: loss improved from 3.12082 to 3.11908, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 19/100\n",
            "1281/1281 [==============================] - 26s 21ms/step - loss: 3.1193 - accuracy: 0.1605\n",
            "\n",
            "Epoch 00019: loss improved from 3.11908 to 3.11878, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 20/100\n",
            "1281/1281 [==============================] - 26s 21ms/step - loss: 3.1210 - accuracy: 0.1606\n",
            "\n",
            "Epoch 00020: loss improved from 3.11878 to 3.11777, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 21/100\n",
            "1281/1281 [==============================] - 26s 20ms/step - loss: 3.1127 - accuracy: 0.1630\n",
            "\n",
            "Epoch 00021: loss improved from 3.11777 to 3.11690, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 22/100\n",
            "1281/1281 [==============================] - 26s 21ms/step - loss: 3.1165 - accuracy: 0.1623\n",
            "\n",
            "Epoch 00022: loss improved from 3.11690 to 3.11518, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 23/100\n",
            "1281/1281 [==============================] - 26s 21ms/step - loss: 3.1160 - accuracy: 0.1613\n",
            "\n",
            "Epoch 00023: loss improved from 3.11518 to 3.11488, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 24/100\n",
            "1281/1281 [==============================] - 26s 21ms/step - loss: 3.1154 - accuracy: 0.1613\n",
            "\n",
            "Epoch 00024: loss did not improve from 3.11488\n",
            "Epoch 25/100\n",
            "1281/1281 [==============================] - 26s 21ms/step - loss: 3.1151 - accuracy: 0.1608\n",
            "\n",
            "Epoch 00025: loss improved from 3.11488 to 3.11374, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 26/100\n",
            "1281/1281 [==============================] - 26s 21ms/step - loss: 3.1049 - accuracy: 0.1642\n",
            "\n",
            "Epoch 00026: loss improved from 3.11374 to 3.11203, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 27/100\n",
            "1281/1281 [==============================] - 26s 21ms/step - loss: 3.1089 - accuracy: 0.1633\n",
            "\n",
            "Epoch 00027: loss did not improve from 3.11203\n",
            "Epoch 28/100\n",
            "1281/1281 [==============================] - 26s 21ms/step - loss: 3.1147 - accuracy: 0.1607\n",
            "\n",
            "Epoch 00028: loss did not improve from 3.11203\n",
            "Epoch 29/100\n",
            "1281/1281 [==============================] - 26s 21ms/step - loss: 3.1131 - accuracy: 0.1632\n",
            "\n",
            "Epoch 00029: loss did not improve from 3.11203\n",
            "Epoch 30/100\n",
            "1281/1281 [==============================] - 26s 21ms/step - loss: 3.1155 - accuracy: 0.1623\n",
            "\n",
            "Epoch 00030: loss did not improve from 3.11203\n",
            "Epoch 31/100\n",
            "1281/1281 [==============================] - 26s 20ms/step - loss: 3.1106 - accuracy: 0.1638\n",
            "\n",
            "Epoch 00031: loss improved from 3.11203 to 3.11133, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 32/100\n",
            "1281/1281 [==============================] - 26s 21ms/step - loss: 3.1114 - accuracy: 0.1628\n",
            "\n",
            "Epoch 00032: loss improved from 3.11133 to 3.11059, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 33/100\n",
            "1281/1281 [==============================] - 26s 20ms/step - loss: 3.1154 - accuracy: 0.1600\n",
            "\n",
            "Epoch 00033: loss improved from 3.11059 to 3.11030, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 34/100\n",
            "1281/1281 [==============================] - 26s 21ms/step - loss: 3.1110 - accuracy: 0.1626\n",
            "\n",
            "Epoch 00034: loss improved from 3.11030 to 3.10849, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 35/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.1069 - accuracy: 0.1641\n",
            "\n",
            "Epoch 00035: loss did not improve from 3.10849\n",
            "Epoch 36/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.1085 - accuracy: 0.1637\n",
            "\n",
            "Epoch 00036: loss improved from 3.10849 to 3.10798, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 37/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.1096 - accuracy: 0.1629\n",
            "\n",
            "Epoch 00037: loss improved from 3.10798 to 3.10776, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 38/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.1129 - accuracy: 0.1634\n",
            "\n",
            "Epoch 00038: loss did not improve from 3.10776\n",
            "Epoch 39/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.1078 - accuracy: 0.1639\n",
            "\n",
            "Epoch 00039: loss did not improve from 3.10776\n",
            "Epoch 40/100\n",
            "1281/1281 [==============================] - 26s 21ms/step - loss: 3.1080 - accuracy: 0.1646\n",
            "\n",
            "Epoch 00040: loss improved from 3.10776 to 3.10743, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 41/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.1068 - accuracy: 0.1626\n",
            "\n",
            "Epoch 00041: loss improved from 3.10743 to 3.10715, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 42/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.1070 - accuracy: 0.1634\n",
            "\n",
            "Epoch 00042: loss improved from 3.10715 to 3.10630, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 43/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.1038 - accuracy: 0.1642\n",
            "\n",
            "Epoch 00043: loss improved from 3.10630 to 3.10576, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 44/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.1029 - accuracy: 0.1648\n",
            "\n",
            "Epoch 00044: loss did not improve from 3.10576\n",
            "Epoch 45/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.1086 - accuracy: 0.1636\n",
            "\n",
            "Epoch 00045: loss did not improve from 3.10576\n",
            "Epoch 46/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.1023 - accuracy: 0.1647\n",
            "\n",
            "Epoch 00046: loss improved from 3.10576 to 3.10537, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 47/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.1121 - accuracy: 0.1624\n",
            "\n",
            "Epoch 00047: loss improved from 3.10537 to 3.10451, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 48/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.1066 - accuracy: 0.1637\n",
            "\n",
            "Epoch 00048: loss improved from 3.10451 to 3.10434, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 49/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.1036 - accuracy: 0.1636\n",
            "\n",
            "Epoch 00049: loss did not improve from 3.10434\n",
            "Epoch 50/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.1048 - accuracy: 0.1634\n",
            "\n",
            "Epoch 00050: loss did not improve from 3.10434\n",
            "Epoch 51/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.1011 - accuracy: 0.1648\n",
            "\n",
            "Epoch 00051: loss did not improve from 3.10434\n",
            "Epoch 52/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.1037 - accuracy: 0.1644\n",
            "\n",
            "Epoch 00052: loss improved from 3.10434 to 3.10341, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 53/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.1052 - accuracy: 0.1656\n",
            "\n",
            "Epoch 00053: loss did not improve from 3.10341\n",
            "Epoch 54/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.1013 - accuracy: 0.1635\n",
            "\n",
            "Epoch 00054: loss did not improve from 3.10341\n",
            "Epoch 55/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.1050 - accuracy: 0.1641\n",
            "\n",
            "Epoch 00055: loss improved from 3.10341 to 3.10296, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 56/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.1014 - accuracy: 0.1662\n",
            "\n",
            "Epoch 00056: loss did not improve from 3.10296\n",
            "Epoch 57/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.1029 - accuracy: 0.1653\n",
            "\n",
            "Epoch 00057: loss did not improve from 3.10296\n",
            "Epoch 58/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.1038 - accuracy: 0.1635\n",
            "\n",
            "Epoch 00058: loss improved from 3.10296 to 3.10173, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 59/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.1007 - accuracy: 0.1650\n",
            "\n",
            "Epoch 00059: loss improved from 3.10173 to 3.10164, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 60/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.0983 - accuracy: 0.1658\n",
            "\n",
            "Epoch 00060: loss improved from 3.10164 to 3.10133, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 61/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.1067 - accuracy: 0.1630\n",
            "\n",
            "Epoch 00061: loss did not improve from 3.10133\n",
            "Epoch 62/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.1026 - accuracy: 0.1648\n",
            "\n",
            "Epoch 00062: loss did not improve from 3.10133\n",
            "Epoch 63/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.1009 - accuracy: 0.1636\n",
            "\n",
            "Epoch 00063: loss did not improve from 3.10133\n",
            "Epoch 64/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.0972 - accuracy: 0.1655\n",
            "\n",
            "Epoch 00064: loss improved from 3.10133 to 3.10132, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 65/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.1038 - accuracy: 0.1640\n",
            "\n",
            "Epoch 00065: loss improved from 3.10132 to 3.10096, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 66/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.1042 - accuracy: 0.1665\n",
            "\n",
            "Epoch 00066: loss improved from 3.10096 to 3.10057, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 67/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.1016 - accuracy: 0.1641\n",
            "\n",
            "Epoch 00067: loss improved from 3.10057 to 3.10030, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 68/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.1015 - accuracy: 0.1641\n",
            "\n",
            "Epoch 00068: loss improved from 3.10030 to 3.10021, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 69/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.0999 - accuracy: 0.1660\n",
            "\n",
            "Epoch 00069: loss did not improve from 3.10021\n",
            "Epoch 70/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.1025 - accuracy: 0.1634\n",
            "\n",
            "Epoch 00070: loss improved from 3.10021 to 3.09983, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 71/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.1006 - accuracy: 0.1641\n",
            "\n",
            "Epoch 00071: loss did not improve from 3.09983\n",
            "Epoch 72/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.0990 - accuracy: 0.1663\n",
            "\n",
            "Epoch 00072: loss did not improve from 3.09983\n",
            "Epoch 73/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.1015 - accuracy: 0.1657\n",
            "\n",
            "Epoch 00073: loss did not improve from 3.09983\n",
            "Epoch 74/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.0988 - accuracy: 0.1659\n",
            "\n",
            "Epoch 00074: loss improved from 3.09983 to 3.09967, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 75/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.0937 - accuracy: 0.1673\n",
            "\n",
            "Epoch 00075: loss improved from 3.09967 to 3.09843, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 76/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.1015 - accuracy: 0.1655\n",
            "\n",
            "Epoch 00076: loss improved from 3.09843 to 3.09832, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 77/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.0990 - accuracy: 0.1655\n",
            "\n",
            "Epoch 00077: loss did not improve from 3.09832\n",
            "Epoch 78/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.0990 - accuracy: 0.1669\n",
            "\n",
            "Epoch 00078: loss did not improve from 3.09832\n",
            "Epoch 79/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.1013 - accuracy: 0.1658\n",
            "\n",
            "Epoch 00079: loss improved from 3.09832 to 3.09794, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 80/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.0975 - accuracy: 0.1653\n",
            "\n",
            "Epoch 00080: loss did not improve from 3.09794\n",
            "Epoch 81/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.1012 - accuracy: 0.1660\n",
            "\n",
            "Epoch 00081: loss did not improve from 3.09794\n",
            "Epoch 82/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.0979 - accuracy: 0.1659\n",
            "\n",
            "Epoch 00082: loss did not improve from 3.09794\n",
            "Epoch 83/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.1010 - accuracy: 0.1645\n",
            "\n",
            "Epoch 00083: loss did not improve from 3.09794\n",
            "Epoch 84/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.1000 - accuracy: 0.1668\n",
            "\n",
            "Epoch 00084: loss improved from 3.09794 to 3.09787, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 85/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.0973 - accuracy: 0.1659\n",
            "\n",
            "Epoch 00085: loss did not improve from 3.09787\n",
            "Epoch 86/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.0938 - accuracy: 0.1675\n",
            "\n",
            "Epoch 00086: loss improved from 3.09787 to 3.09781, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 87/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.0976 - accuracy: 0.1660\n",
            "\n",
            "Epoch 00087: loss improved from 3.09781 to 3.09724, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 88/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.0993 - accuracy: 0.1651\n",
            "\n",
            "Epoch 00088: loss did not improve from 3.09724\n",
            "Epoch 89/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.0981 - accuracy: 0.1657\n",
            "\n",
            "Epoch 00089: loss did not improve from 3.09724\n",
            "Epoch 90/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.0992 - accuracy: 0.1671\n",
            "\n",
            "Epoch 00090: loss did not improve from 3.09724\n",
            "Epoch 91/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.0981 - accuracy: 0.1647\n",
            "\n",
            "Epoch 00091: loss improved from 3.09724 to 3.09685, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 92/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.0960 - accuracy: 0.1666\n",
            "\n",
            "Epoch 00092: loss improved from 3.09685 to 3.09653, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 93/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.0981 - accuracy: 0.1656\n",
            "\n",
            "Epoch 00093: loss did not improve from 3.09653\n",
            "Epoch 94/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.1020 - accuracy: 0.1657\n",
            "\n",
            "Epoch 00094: loss did not improve from 3.09653\n",
            "Epoch 95/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.0936 - accuracy: 0.1666\n",
            "\n",
            "Epoch 00095: loss improved from 3.09653 to 3.09591, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n",
            "Epoch 96/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.0953 - accuracy: 0.1672\n",
            "\n",
            "Epoch 00096: loss did not improve from 3.09591\n",
            "Epoch 97/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.1019 - accuracy: 0.1659\n",
            "\n",
            "Epoch 00097: loss did not improve from 3.09591\n",
            "Epoch 98/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.0947 - accuracy: 0.1666\n",
            "\n",
            "Epoch 00098: loss did not improve from 3.09591\n",
            "Epoch 99/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.0957 - accuracy: 0.1667\n",
            "\n",
            "Epoch 00099: loss did not improve from 3.09591\n",
            "Epoch 100/100\n",
            "1281/1281 [==============================] - 27s 21ms/step - loss: 3.0958 - accuracy: 0.1660\n",
            "\n",
            "Epoch 00100: loss improved from 3.09591 to 3.09488, saving model to /content/drive/My Drive/Mastering Keras/alice_best_weights.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-b804c79b2fd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-ff5de42a1d4c>\u001b[0m in \u001b[0;36mfit_model\u001b[0;34m(model, X, Y, epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# load the best weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;31m# return the final model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'filename' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clAYq_A81Cob"
      },
      "source": [
        "Here we will load saved weights. You can use the \"alice_best_weights.hdf5\" file that comes with the course - just place it in the same folder as the \"alice.txt\" file in your google drive. This file has been trained for 200 epoches, and gets a loss around 1.16.\n",
        "\n",
        "If you train the network yourself, the best weights will be saved as \"alice_best_weights.hdf5\" in the same location as above. You can therefore use the same code in both cases.\n",
        "\n",
        "In all cases remember to change the filepath if you are not using the default folder name.\n",
        "\n",
        "If you are resuming this tutorial here in a new session, you should re-mount your Google drive using the earlier code, re-load the data, and then run this code block to load the weights into a new model. \n",
        "\n",
        "If you want to train the model further, you will need to compile it with an optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7bvAwsEfzlo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "outputId": "2d48bd39-c481-4caa-e8dd-a49b2b2903cf"
      },
      "source": [
        "model=get_model(X,Y)\n",
        "filepath=\"/content/drive/My Drive/Mastering Keras Datasets/alice_best_weights.hdf5\"\n",
        "model.load_weights(filepath)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1x9CiNJ1tjY"
      },
      "source": [
        "Now we can see if our network has mastered the art of writing like Lewis Carroll! Let's write a function to let us see, and then call it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC2dzmdpITMG"
      },
      "source": [
        "def write_like_Lewis_Carroll(model,X_,n_vocab,int_to_char):\n",
        "  # pick a random seed...\n",
        "  start = numpy.random.randint(0, len(X_)-1)\n",
        "  # ... in order to decide which X datum to use to start\n",
        "  pattern = X_[start]\n",
        "\n",
        "  print (\"Seed:\")\n",
        "  print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "  # generate characters\n",
        "  for i in range(1000):\n",
        "    # We transform the integer mapping of the characters to\n",
        "    # real numbers suitable for input into our model.\n",
        "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "    x = x/float(n_vocab)\n",
        "    # We use the model to estimate the probability distribution for\n",
        "    # the next character\n",
        "    prediction = model.predict(x, verbose=0)\n",
        "    # We choose as the next character whichever the model thinks is most likely\n",
        "    index = numpy.argmax(prediction)\n",
        "    result = int_to_char[index]\n",
        "    seq_in = [int_to_char[value] for value in pattern]\n",
        "    sys.stdout.write(result)\n",
        "    # We add the integer to our pattern... \n",
        "    pattern.append(index)\n",
        "    # ... and drop the earliest integer from our pattern.\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "  print (\"\\nDone.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aG-PGS0vISCB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "8896db20-cfed-4a7a-bbfc-c539508b3446"
      },
      "source": [
        "write_like_Lewis_Carroll(model,X_,n_vocab,int_to_char)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" for it to speak with.\n",
            "\n",
            "alice waited till the eyes appeared, and then nodded. 'it's no use\n",
            "speaking t \"\n",
            "o see the mock turtle shat '\n",
            "\n",
            "'i should hiv tereat ' thought alice, 'i must be giederen seams to be a bonk,' she said to herself, 'it would be of very curious to onow what there was a sery dortut, and the ooral of that iss thin the cook and a large rister sha thought the was now one of the court.\n",
            "but the dould not heve a little botrle of the thate with a things of tee the door, she could not hear the conlers on the coor with pisted so see it was she same sotnd and mook up and was that it was ouer the whnle shoiek, and the thought the was now a bot of ceain, and was domencd it voice and bookdrs shat the was nuire silent for a minute, and she was nooiing at the court.\n",
            "\n",
            "'i should hit tere things,' said the caterpillar.\n",
            "\n",
            "'well, perhaps you may bean the same siings tuertion,' the duchess said to the gryphon.\n",
            "\n",
            "'what i cen the thing,' said the caterpillar.\n",
            "\n",
            "'well, perhaps you may bean the same siings tuertion,' the mock turtle seplied,\n",
            "\n",
            "'that i man the mice,' said the caterpillar.\n",
            "\n",
            "'well, per\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BG_f2DsQgW5_"
      },
      "source": [
        "If you run the above a few times, you will see that we have had some success - though we are still a long way from a good Alice in Wonderland simulator!\n",
        "\n",
        "Here is an extract from one simulation I ran:\n",
        "\n",
        "*'i should hit tere things,' said the caterpillar.*\n",
        "\n",
        "*'well, perhaps you may bean the same siings tuertion,' the duchess said to the gryphon.*\n",
        "\n",
        "*'what i cen the thing,' said the caterpillar.*\n",
        "\n",
        "*'well, perhaps you may bean the same siings tuertion,' the mock turtle seplied,*\n",
        "\n",
        "*'that i man the mice,' said the caterpillar.*\n",
        "\n",
        "We have got to the point of basic sentence structure, quotations for speech, plausible characters given the context, etc. There remains misspellings, and occasional punctuation errors, and other issues. (And this was a good selection.) \n",
        "\n",
        "In fact, you should be able to do much better. Trying with 500 time points (predicting the 501st character from the preceeding 500) and using a three layer LSTM will lead to major improvements. So would using more training data (multiple Lewis Carole books). You can see the performance achieved on a Shakespeare simulator [here](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). \n",
        "\n",
        "If you have time, consider it an exercise to try to improve this implementation to that level - but be warned, the suggested changes would lead to training time being about 7 times longer for the same number of epochs, and of course more epoches would be required as it would be a more complex model. Since it would have taken 100+ hours on the Colab environment (which disconnects after a time limit) this is really only an exercise for those with access to a powerful local environment. "
      ]
    }
  ]
}